# ComfyUI 模型加载系统 - sd.py 深度解析

## 概述

`comfy/sd.py` 是 ComfyUI 的核心模型加载模块，负责加载和管理 Stable Diffusion 系列模型的三大组件：
- **Diffusion Model (UNet/DiT)**: 扩散模型主体
- **CLIP**: 文本编码器
- **VAE**: 变分自编码器

文件约 1540 行，是理解模型加载流程的关键。

## 核心架构

```
load_checkpoint_guess_config()
    ├── comfy.utils.load_torch_file()          # 加载权重文件
    ├── model_detection.unet_prefix_from_state_dict()  # 检测前缀
    ├── model_detection.model_config_from_unet()       # 检测模型类型
    ├── model_config.get_model()               # 创建模型实例
    ├── VAE(sd=vae_sd)                         # 创建VAE
    ├── CLIP(clip_target, ...)                 # 创建CLIP
    └── ModelPatcher()                         # 包装为可热加载的模型
```

## 一、模型加载入口函数

### 1.1 load_checkpoint_guess_config (sd.py:1300)

主要的 checkpoint 加载函数，自动检测模型类型：

```python
def load_checkpoint_guess_config(ckpt_path, output_vae=True, output_clip=True,
                                  output_clipvision=False, embedding_directory=None,
                                  output_model=True, model_options={}, te_model_options={}):
    # 1. 加载权重文件
    sd, metadata = comfy.utils.load_torch_file(ckpt_path, return_metadata=True)

    # 2. 调用状态字典加载函数
    out = load_state_dict_guess_config(sd, output_vae, output_clip, ...)

    return out  # (model_patcher, clip, vae, clipvision)
```

### 1.2 load_state_dict_guess_config (sd.py:1307)

从状态字典推断配置并加载模型：

```python
def load_state_dict_guess_config(sd, output_vae=True, output_clip=True, ...):
    # 1. 检测 UNet 前缀
    diffusion_model_prefix = model_detection.unet_prefix_from_state_dict(sd)
    # 可能的前缀: "model.diffusion_model.", "model.model.", "net.", "model."

    # 2. 计算参数量和权重类型
    parameters = comfy.utils.calculate_parameters(sd, diffusion_model_prefix)
    weight_dtype = comfy.utils.weight_dtype(sd, diffusion_model_prefix)

    # 3. 从 UNet 配置检测模型类型
    model_config = model_detection.model_config_from_unet(sd, diffusion_model_prefix)

    # 4. 配置推理数据类型
    unet_dtype = model_management.unet_dtype(model_params=parameters, ...)
    model_config.set_inference_dtype(unet_dtype, manual_cast_dtype)

    # 5. 创建模型实例
    if output_model:
        model = model_config.get_model(sd, diffusion_model_prefix, device=inital_load_device)
        model.load_model_weights(sd, diffusion_model_prefix)

    # 6. 创建 VAE
    if output_vae:
        vae_sd = comfy.utils.state_dict_prefix_replace(sd, {k: "" for k in model_config.vae_key_prefix})
        vae = VAE(sd=vae_sd, metadata=metadata)

    # 7. 创建 CLIP
    if output_clip:
        clip_target = model_config.clip_target(state_dict=sd)
        clip = CLIP(clip_target, embedding_directory=embedding_directory, ...)

    # 8. 包装为 ModelPatcher
    if output_model:
        model_patcher = comfy.model_patcher.ModelPatcher(model, load_device=load_device,
                                                          offload_device=model_management.unet_offload_device())

    return (model_patcher, clip, vae, clipvision)
```

### 1.3 load_diffusion_model (sd.py:1499)

单独加载扩散模型（不含 VAE/CLIP）：

```python
def load_diffusion_model(unet_path, model_options={}):
    sd, metadata = comfy.utils.load_torch_file(unet_path, return_metadata=True)
    model = load_diffusion_model_state_dict(sd, model_options=model_options, metadata=metadata)
    return model  # ModelPatcher
```

## 二、模型类型检测 (model_detection.py)

### 2.1 unet_prefix_from_state_dict

检测权重字典中的模型前缀：

```python
def unet_prefix_from_state_dict(state_dict):
    candidates = [
        "model.diffusion_model.",  # LDM/SGM 模型 (SD1.x, SD2.x, SDXL)
        "model.model.",            # 音频模型
        "net.",                    # Cosmos
    ]
    # 统计每个前缀出现的次数，返回最多的
    # 默认返回 "model." (用于 AuraFlow 等)
```

### 2.2 detect_unet_config

根据权重键名检测模型架构类型：

```python
def detect_unet_config(state_dict, key_prefix, metadata=None):
    state_dict_keys = list(state_dict.keys())

    # MMDiT 模型 (SD3, Flux)
    if 'joint_blocks.0.context_block.attn.qkv.weight' in keys:
        return mmdit_config

    # Stable Cascade
    if 'clf.1.weight' in keys:
        return cascade_config

    # Stable Audio DiT
    if 'transformer.rotary_pos_emb.inv_freq' in keys:
        return audio_dit_config

    # AuraFlow DiT
    if 'double_layers.0.attn.w1q.weight' in keys:
        return aura_config

    # Hunyuan DiT
    if 'mlp_t5.0.weight' in keys:
        return hydit_config

    # Hunyuan Video
    if 'txt_in.individual_token_refiner.blocks.0.norm1.weight' in keys:
        return hunyuan_video_config

    # Flux / Chroma
    if 'double_blocks.0.img_attn.norm.key_norm.scale' in keys:
        return flux_config

    # 传统 UNet (SD1.x, SD2.x, SDXL)
    return detect_standard_unet_config(state_dict, key_prefix)
```

### 2.3 model_config_from_unet_config

将检测到的配置匹配到具体的模型类：

```python
def model_config_from_unet_config(unet_config, state_dict=None):
    for model_config in comfy.supported_models.models:
        if model_config.matches(unet_config, state_dict):
            return model_config(unet_config)
    return None
```

支持的模型类型（在 `comfy/supported_models.py`）：
- SD15, SD20, SD21, SD21UnCLIP
- SDXLBase, SDXLRefiner
- SD3, Flux, FluxSchnell
- HunyuanDiT, HunyuanVideo
- StableCascade_C, StableCascade_B
- AuraFlow, Cosmos, Lumina2
- 等等...

## 三、CLIP 类 (sd.py:101)

### 3.1 初始化

```python
class CLIP:
    def __init__(self, target=None, embedding_directory=None, no_init=False,
                 tokenizer_data={}, parameters=0, state_dict=[], model_options={}):
        # 1. 获取设备配置
        load_device = model_management.text_encoder_device()
        offload_device = model_management.text_encoder_offload_device()

        # 2. 确定数据类型
        dtype = model_management.text_encoder_dtype(load_device)

        # 3. 创建条件阶段模型 (实际的 CLIP/T5 等)
        self.cond_stage_model = clip(**(params))

        # 4. 创建分词器
        self.tokenizer = tokenizer(embedding_directory=embedding_directory, tokenizer_data=tokenizer_data)

        # 5. 包装为 ModelPatcher
        self.patcher = comfy.model_patcher.ModelPatcher(self.cond_stage_model,
                                                         load_device=load_device,
                                                         offload_device=offload_device)

        # 6. 加载权重
        if len(state_dict) > 0:
            self.load_sd(state_dict)
```

### 3.2 CLIPType 枚举 (sd.py 约 1070 行)

定义支持的 CLIP 类型：

```python
class CLIPType(Enum):
    STABLE_DIFFUSION = 1      # SD1.x CLIP-L
    STABLE_CASCADE = 2        # Stable Cascade
    SD3 = 3                   # SD3 双CLIP (CLIP-L + CLIP-G + T5)
    STABLE_AUDIO = 4          # 音频模型
    HUNYUAN_DIT = 5           # HunyuanDiT (中文)
    FLUX = 6                  # Flux (CLIP-L + T5-XXL)
    MOCHI = 7                 # Mochi 视频
    LTXV = 8                  # LTXV 视频
    HUNYUAN_VIDEO = 9         # HunyuanVideo
    PIXART = 10               # PixArt T5
    WAN = 11                  # Wan T5
    HIDREAM = 12              # HiDream
```

### 3.3 编码方法

```python
def encode_from_tokens(self, tokens, return_pooled=False, ...):
    """将 token 编码为条件向量"""
    self.load_model()  # 确保模型在 GPU
    cond, pooled = self.cond_stage_model.encode_token_weights(tokens)

    if return_pooled:
        return cond, self.add_hooks_to_dict({"pooled_output": pooled})
    return cond
```

## 四、VAE 类 (sd.py:309)

### 4.1 初始化与自动检测

VAE 类会根据权重键名自动检测 VAE 类型：

```python
class VAE:
    def __init__(self, sd=None, device=None, config=None, dtype=None, metadata=None):
        # 默认配置
        self.latent_dim = 2
        self.latent_channels = 4
        self.output_channels = 3
        self.upscale_ratio = 8
        self.downscale_ratio = 8

        if config is None:
            # 自动检测 VAE 类型
            if "decoder.up_blocks.0.resnets.0.norm1.weight" in sd:
                # SDXL/Flux 风格 VAE
                self.first_stage_model = AutoencodingEngine(...)

            elif "decoder.layers.1.layers.0.beta" in sd:
                # Audio Oobleck VAE
                self.first_stage_model = AudioOobleckVAE()

            elif "blocks.2.blocks.3.stack.5.weight" in sd:
                # Mochi Video VAE
                self.first_stage_model = comfy.ldm.genmo.vae.model.VideoVAE()

            elif "decoder.up_blocks.0.res_blocks.0.conv1.conv.weight" in sd:
                # LTXV Video VAE
                self.first_stage_model = comfy.ldm.lightricks.vae.VideoVAE(...)

            elif "decoder.unpatcher3d.wavelets" in sd:
                # Cosmos VAE
                self.first_stage_model = comfy.ldm.cosmos.vae.CausalContinuousVideoTokenizer(...)

            elif "decoder.middle.0.residual.0.gamma" in sd:
                # Wan VAE (2.1 或 2.2)
                self.first_stage_model = comfy.ldm.wan.vae.WanVAE(...)

            else:
                # 标准 SD1.x/SD2.x VAE
                self.first_stage_model = AutoencoderKL(ddconfig=ddconfig, ...)
```

### 4.2 关键属性

不同 VAE 类型有不同的属性：

| VAE 类型 | latent_channels | upscale_ratio | latent_dim |
|---------|-----------------|---------------|------------|
| SD 1.x/2.x | 4 | 8 | 2 (图像) |
| SDXL/Flux | 4/16 | 8 | 2 |
| Mochi Video | 12 | (6,8,8) | 3 (视频) |
| LTXV Video | 128 | (8,32,32) | 3 |
| Hunyuan Video | 16 | (4,8,8) | 3 |
| Cosmos | 16 | (8,8,8) | 3 |
| Wan 2.1 | 16 | (4,8,8) | 3 |
| Audio | 64 | 2048 | 1 |

### 4.3 编码和解码

```python
def decode(self, samples_in):
    """将潜在空间样本解码为图像/视频"""
    self.throw_exception_if_invalid()

    # 计算所需显存
    memory_used = self.memory_used_decode(samples.shape, self.vae_dtype)

    # 加载模型到 GPU
    model_management.load_models_gpu([self.patcher], memory_required=memory_used)

    # 处理输入（应用缩放因子）
    samples = self.process_input(samples)

    # 解码
    output = self.first_stage_model.decode(samples)

    return output.movedim(1, -1)  # BCHW -> BHWC

def encode(self, pixel_samples):
    """将图像/视频编码为潜在空间样本"""
    pixel_samples = self.vae_encode_crop_pixels(pixel_samples)
    pixel_samples = pixel_samples.movedim(-1, 1)  # BHWC -> BCHW

    # 编码
    samples = self.first_stage_model.encode(pixel_samples)

    return self.process_output(samples)
```

### 4.4 分块处理 (Tiled Processing)

对于大尺寸图像，使用分块处理避免 OOM：

```python
def decode_tiled(self, samples, tile_x=64, tile_y=64, overlap=16):
    """分块解码，避免显存不足"""
    steps = samples.shape[0] * comfy.utils.get_tiled_scale_steps(...)
    pbar = ProgressBar(steps)

    decode_fn = lambda a: self.first_stage_model.decode(...)
    output = comfy.utils.tiled_scale(samples, decode_fn,
                                      tile_x * 2, tile_y, overlap * 2,
                                      upscale_amount=self.upscale_ratio,
                                      pbar=pbar)
    return output
```

## 五、LoRA 加载 (sd.py:70)

```python
def load_lora_for_models(model, clip, lora, strength_model, strength_clip):
    """为模型和 CLIP 加载 LoRA"""

    # 1. 获取模型的 LoRA 键映射
    key_map = {}
    if model is not None:
        key_map = comfy.lora.model_lora_keys_unet(model.model, key_map)
    if clip is not None:
        key_map = comfy.lora.model_lora_keys_clip(clip.cond_stage_model, key_map)

    # 2. 转换 LoRA 格式
    lora = comfy.lora_convert.convert_lora(lora)

    # 3. 加载 LoRA 权重
    loaded = comfy.lora.load_lora(lora, key_map)

    # 4. 应用到模型（克隆后添加补丁）
    if model is not None:
        new_modelpatcher = model.clone()
        new_modelpatcher.add_patches(loaded, strength_model)

    if clip is not None:
        new_clip = clip.clone()
        new_clip.add_patches(loaded, strength_clip)

    return (new_modelpatcher, new_clip)
```

## 六、Checkpoint 保存 (sd.py:1515)

```python
def save_checkpoint(output_path, model, clip=None, vae=None, clip_vision=None, metadata=None, extra_keys={}):
    """保存模型到 checkpoint 文件"""

    # 收集需要保存的模型
    load_models = [model]
    if clip is not None:
        load_models.append(clip.load_model())
        clip_sd = clip.get_sd()
    if vae is not None:
        vae_sd = vae.get_sd()

    # 确保模型在 GPU 上并应用补丁
    model_management.load_models_gpu(load_models, force_patch_weights=True)

    # 获取模型状态字典
    sd = model.model.state_dict_for_saving(clip_sd, vae_sd, clip_vision_sd)

    # 确保张量连续
    for k in sd:
        if not sd[k].is_contiguous():
            sd[k] = sd[k].contiguous()

    # 保存
    comfy.utils.save_torch_file(sd, output_path, metadata=metadata)
```

## 七、模型加载流程图

```
用户选择 .safetensors 文件
        │
        ▼
┌─────────────────────────────┐
│  load_checkpoint_guess_config │
│  或 load_diffusion_model     │
└──────────────┬──────────────┘
               │
        ▼
┌─────────────────────────────┐
│  comfy.utils.load_torch_file │
│  (加载权重到 CPU)            │
└──────────────┬──────────────┘
               │
        ▼
┌─────────────────────────────┐
│  model_detection            │
│  ├─ unet_prefix_from_sd()  │
│  ├─ detect_unet_config()   │
│  └─ model_config_from_unet()│
└──────────────┬──────────────┘
               │
        ▼
┌─────────────────────────────┐
│  创建模型实例                │
│  ├─ model_config.get_model()│
│  ├─ VAE(sd=vae_sd)         │
│  └─ CLIP(clip_target, ...)  │
└──────────────┬──────────────┘
               │
        ▼
┌─────────────────────────────┐
│  加载权重                    │
│  ├─ model.load_model_weights()│
│  ├─ vae.load_state_dict()   │
│  └─ clip.load_sd()          │
└──────────────┬──────────────┘
               │
        ▼
┌─────────────────────────────┐
│  包装为 ModelPatcher         │
│  (支持热加载和 LoRA)         │
└─────────────────────────────┘
```

## 八、设备管理策略

ComfyUI 使用智能的设备管理策略：

### 8.1 初始加载设备

```python
# 大模型：先加载到 CPU
inital_load_device = model_management.unet_inital_load_device(parameters, unet_dtype)

# 小模型或显存充足：直接加载到 GPU
if inital_load_device != torch.device("cpu"):
    logging.info("loaded diffusion model directly to GPU")
    model_management.load_models_gpu([model_patcher], force_full_load=True)
```

### 8.2 数据类型选择

```python
# 根据显卡能力自动选择
unet_dtype = model_management.unet_dtype(
    model_params=parameters,           # 模型参数量
    supported_dtypes=unet_weight_dtype, # 模型支持的类型
    weight_dtype=weight_dtype           # 权重实际类型
)
# 可能返回: torch.float16, torch.bfloat16, torch.float32
```

## 九、特殊模型支持

### 9.1 视频模型

视频模型的 VAE 具有 3D 潜在空间：

```python
# 视频 VAE 属性示例 (Hunyuan Video)
self.latent_dim = 3  # (帧, 高, 宽)
self.upscale_ratio = (lambda a: max(0, a * 4 - 3), 8, 8)  # 时间:4, 空间:8
self.downscale_ratio = (lambda a: max(0, math.floor((a + 3) / 4)), 8, 8)
```

### 9.2 音频模型

音频模型使用 1D 潜在空间：

```python
# Audio VAE 属性示例
self.latent_dim = 1
self.upscale_ratio = 2048
self.output_channels = 2  # 立体声
```

### 9.3 3D 模型 (Hunyuan3D)

```python
# 3D VAE 属性
self.latent_dim = 1
# 特殊的内存估算考虑 KV 缓存
self.memory_used_decode = lambda shape, dtype, num_layers=16, kv_cache_multiplier=2: ...
```

## 十、量化支持

sd.py 支持混合精度量化：

```python
# 检测量化配置
sd, metadata = comfy.utils.convert_old_quants(sd, diffusion_model_prefix, metadata=metadata)

# 检测逐层量化
quant_config = comfy.utils.detect_layer_quantization(state_dict, unet_key_prefix)
if quant_config:
    model_config.quant_config = quant_config
    logging.info("Detected mixed precision quantization")
```

## 总结

sd.py 是 ComfyUI 模型加载的核心，主要功能：

1. **自动检测**: 通过权重键名自动识别模型类型（SD1.x/SD2.x/SDXL/Flux/SD3等）
2. **统一接口**: 为不同模型提供统一的加载接口
3. **智能设备管理**: 根据显存自动选择加载策略
4. **组件分离**: 支持单独加载 UNet/CLIP/VAE
5. **格式兼容**: 支持 diffusers 格式转换
6. **LoRA 支持**: 通过 ModelPatcher 实现动态 LoRA 加载
7. **多媒体支持**: 图像、视频、音频、3D 模型统一处理
