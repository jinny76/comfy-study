# ComfyUI 性能优化指南

## 显存管理

### 启动参数选择

根据显存大小选择合适的启动参数：

| 显存 | 参数 | 说明 |
|------|------|------|
| 4GB | `--lowvram` | 模型分块加载，最省显存 |
| 6GB | `--lowvram` | 较慢但能跑大多数模型 |
| 8GB | `--normalvram` | 默认模式 |
| 12GB | `--normalvram` | 可跑 Flux 量化版 |
| 16GB+ | `--highvram` | 模型常驻显存，切换快 |
| 24GB+ | `--highvram --gpu-only` | 全部在 GPU 运行 |

### 各参数详解

**--lowvram**
- 将 UNet 分块，每次只加载一部分到显存
- 速度较慢，但显存占用最低
- 适合跑 SDXL 在 6GB 显卡上

**--normalvram** (默认)
- 整个 UNet 加载到显存
- 不用时自动卸载到内存
- 平衡速度和显存

**--highvram**
- 模型保持在显存中不卸载
- 切换模型更快
- 适合显存充足时

**--gpu-only**
- 所有张量都在 GPU
- 需要大显存
- 与 --highvram 配合使用

### 显存不足的解决方案

1. **降低图像尺寸**
   - SD 1.5: 512x512 最省
   - SDXL: 768x768 比 1024x1024 省很多

2. **减少批量大小**
   - batch_size 设为 1

3. **使用量化模型**
   - Flux GGUF 量化版
   - 8GB 可跑 Flux Q4

4. **启用 FP8/FP16**
   - 精度换显存

5. **关闭预览**
   - 预览会占用额外显存

## 速度优化

### 1. 安装 xformers

```bash
pip install xformers
```

xformers 提供优化的注意力计算，显著提速。

### 2. 使用 PyTorch 2.0+ 编译

PyTorch 2.0 的 `torch.compile` 可以加速：

```python
# 在节点中可能已自动启用
```

### 3. 选择合适的采样器

| 采样器 | 速度 | 质量 |
|--------|------|------|
| euler | 最快 | 一般 |
| euler_ancestral | 快 | 多样 |
| dpmpp_2m | 中等 | 好 |
| dpmpp_sde | 慢 | 很好 |
| uni_pc | 快 | 好 |

**推荐**:
- 快速出图: `euler` 或 `uni_pc`
- 质量优先: `dpmpp_2m` + `karras`

### 4. 减少采样步数

| 模型 | 推荐步数 |
|------|----------|
| SD 1.5 | 20-25 |
| SDXL | 25-30 |
| Flux Schnell | 4 |
| Flux Dev | 20-28 |
| LCM 模型 | 4-8 |

### 5. 使用加速模型

- **LCM**: 4-8 步即可出图
- **Turbo**: SDXL Turbo 1步出图
- **Lightning**: 加速版模型
- **Flux Schnell**: 4步高质量

### 6. 预览优化

```bash
# 使用 TAESD 预览 (更快)
python main.py --preview-method taesd

# 使用 latent2rgb (最快但质量差)
python main.py --preview-method latent2rgb

# 禁用预览
python main.py --preview-method none
```

## 批量处理

### 1. 使用 batch_size

Empty Latent Image 节点设置 `batch_size`:
- 一次生成多张，比多次生成单张快
- 但会占用更多显存

### 2. 队列多个任务

点击多次 "Queue Prompt"，任务会排队执行。

### 3. 自动队列

菜单中开启 "Auto Queue"，完成后自动开始下一个。

### 4. API 批量调用

```python
import json
import urllib.request

def queue_prompt(prompt):
    data = json.dumps({"prompt": prompt}).encode('utf-8')
    req = urllib.request.Request("http://127.0.0.1:8188/prompt", data=data)
    urllib.request.urlopen(req)
```

## 工作流优化

### 1. 利用缓存

ComfyUI 自动缓存未改变的节点输出：
- 只改提示词 → 模型不重新加载
- 只改种子 → CLIP 编码不重复

### 2. 避免重复计算

- 相同的 CLIP 编码可以连接到多个 KSampler
- 一次加载模型，多次使用

### 3. 使用 Primitive 节点

用 Primitive 节点统一控制多个参数，避免手动逐个修改。

### 4. 分组管理

将相关节点打组 (Group)：
- 选中节点 → 右键 → Add Group
- 方便折叠和管理

## FP8 量化

### 启用 FP8

部分节点支持 FP8 精度：
- 显存占用减半
- 速度可能更快
- 质量轻微下降

```python
# 在支持的节点中选择 fp8_e4m3fn 或 fp8_e5m2
```

### GGUF 量化模型

Flux 等大模型的 GGUF 版本：
- Q8_0: 质量接近原版
- Q5_K: 平衡
- Q4_K: 省显存

使用 GGUF 加载节点加载。

## 多 GPU

### 指定 GPU

```bash
# 使用第二块显卡
python main.py --cuda-device 1

# 或通过环境变量
CUDA_VISIBLE_DEVICES=1 python main.py
```

### 模型分布

某些节点支持将模型分配到不同 GPU。

## 内存优化

### 系统内存

- 16GB: 基本够用
- 32GB: 更舒适
- 64GB: 大模型无压力

### 磁盘缓存

模型首次加载后会缓存，后续加载更快。

确保系统盘有足够空间。

## 监控工具

### 显存监控

```bash
# Windows
nvidia-smi -l 1

# 或使用 GPU-Z, MSI Afterburner
```

### ComfyUI 状态栏

界面底部显示：
- 显存使用
- 执行进度
- 当前节点

### 日志查看

启动终端会显示详细日志，包括：
- 模型加载时间
- 各节点执行时间
- 错误信息

## 常见性能问题

### Q: 为什么第一次生成特别慢？

模型首次加载需要时间，后续会使用缓存。

### Q: 切换模型太慢？

使用 `--highvram` 参数让模型常驻显存。

### Q: 生成中途卡住？

可能是：
1. 预览渲染中
2. 模型在显存和内存间交换
3. 节点在处理中

### Q: 显存占用越来越高？

重启 ComfyUI 释放显存，或点击菜单中的 "Free Memory"。

## 性能对比参考

以 RTX 3060 12GB 为例：

| 模型 | 尺寸 | 步数 | 时间 |
|------|------|------|------|
| SD 1.5 | 512x512 | 20 | ~3s |
| SD 1.5 | 768x768 | 20 | ~6s |
| SDXL | 1024x1024 | 25 | ~15s |
| Flux Dev | 1024x1024 | 20 | ~25s |
| Flux Schnell | 1024x1024 | 4 | ~8s |

*实际时间因硬件和配置而异*

## 总结

优化优先级建议：

1. **选择合适的显存模式** - 立竿见影
2. **安装 xformers** - 免费提速
3. **使用合适的采样器和步数** - 时间减半
4. **考虑量化模型** - 大模型必备
5. **合理设计工作流** - 避免重复计算
